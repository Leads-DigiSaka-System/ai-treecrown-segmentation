{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq3kEjEmpHA5",
        "outputId": "f7118c5a-2f67-435b-81fe-54c5a32f65a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (1.5.0)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.4.8)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2026.1.4)\n",
            "Requirement already satisfied: click!=8.2.*,>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.3.1)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.1.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch<2.10,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.10,>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.10,>=1.8.0->ultralytics) (3.0.3)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-opc59tia\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-opc59tia\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.32)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: absl-py~=2.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (2.4.0)\n",
            "Requirement already satisfied: sounddevice~=0.5 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.5)\n",
            "Requirement already satisfied: flatbuffers~=25.9 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.12.19)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->sounddevice~=0.5->mediapipe) (2.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio geopandas ultralytics\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FbNHtU7LMvp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dask import\n",
        "try:\n",
        "    import dask.bag as db\n",
        "    from dask.diagnostics import ProgressBar\n",
        "    DASK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DASK_AVAILABLE = False\n",
        "    print(\"⚠ Dask not available, using sequential processing\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "from matplotlib.collections import PatchCollection\n",
        "from matplotlib.patches import Polygon as MPLPolygon\n",
        "from shapely.geometry import Point, Polygon, MultiPolygon\n",
        "\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# Set up logging\n",
        "import time\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectionState:\n",
        "    \"\"\"State container for detection pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, image_path, model, config):\n",
        "        self.image_path = image_path\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "\n",
        "        # State variables\n",
        "        self.full_image = None\n",
        "        self.original_dims = None\n",
        "        self.resized_image = None\n",
        "        self.scaled_dims = None\n",
        "        self.transform = None\n",
        "        self.crs = None\n",
        "\n",
        "        # Detection state\n",
        "        self.boxes_list = []\n",
        "        self.segmentation_results = []\n",
        "        self.gdf = None\n",
        "        self.detection_info = []\n",
        "\n",
        "        # SAM state\n",
        "        self.sam_predictor = None\n",
        "\n",
        "        # Timing\n",
        "        self.start_time = time.process_time()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"DetectionState(boxes={len(self.boxes_list)}, \"\n",
        "                f\"segments={len(self.segmentation_results)}, \"\n",
        "                f\"gdf_size={len(self.gdf) if self.gdf is not None else 0})\")\n",
        "\n",
        "\n",
        "class StateTransition:\n",
        "    \"\"\"Base class for state transitions\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class LoadImageTransition(StateTransition):\n",
        "    \"\"\"Transition: Load and prepare image\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        try:\n",
        "            with rasterio.open(state.image_path) as src:\n",
        "                state.original_dims = (src.width, src.height)\n",
        "                state.transform = src.transform\n",
        "                state.crs = src.crs\n",
        "\n",
        "                print(f\"Original image size: {state.original_dims[0]}x{state.original_dims[1]}\")\n",
        "\n",
        "                if src.count >= 3:\n",
        "                    state.full_image = src.read([1, 2, 3])\n",
        "                    state.full_image = np.transpose(state.full_image, (1, 2, 0))\n",
        "                else:\n",
        "                    state.full_image = src.read(1)\n",
        "                    if len(state.full_image.shape) == 2:\n",
        "                        state.full_image = cv2.cvtColor(state.full_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "                state.full_image = np.clip(state.full_image, 0, 255).astype(np.uint8)\n",
        "        except:\n",
        "            print(\"Loading as regular image (not GeoTIFF)...\")\n",
        "            state.full_image = cv2.imread(state.image_path)\n",
        "            if state.full_image is None:\n",
        "                raise ValueError(f\"Could not load image from {state.image_path}\")\n",
        "\n",
        "            state.full_image = cv2.cvtColor(state.full_image, cv2.COLOR_BGR2RGB)\n",
        "            h, w = state.full_image.shape[:2]\n",
        "            state.original_dims = (w, h)\n",
        "\n",
        "            print(f\"Original image size: {w}x{h}\")\n",
        "\n",
        "            state.transform = from_bounds(0, 0, w, h, w, h)\n",
        "            state.crs = None\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class ResizeImageTransition(StateTransition):\n",
        "    \"\"\"Transition: Resize image based on resolution config\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        resolution = state.config['resolution']\n",
        "        w, h = state.original_dims\n",
        "\n",
        "        new_w = int(w * resolution)\n",
        "        new_h = int(h * resolution)\n",
        "        state.scaled_dims = (new_w, new_h)\n",
        "\n",
        "        print(f\"Processing at: {new_w}x{new_h} (resolution={resolution})\")\n",
        "\n",
        "        state.resized_image = cv2.resize(\n",
        "            state.full_image,\n",
        "            (new_w, new_h),\n",
        "            interpolation=cv2.INTER_LINEAR\n",
        "        )\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class YOLODetectionTransition(StateTransition):\n",
        "    \"\"\"Transition: Perform YOLO detection on tiles\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        tile_size = state.config['tile_size']\n",
        "        overlap = state.config['overlap']\n",
        "        conf_threshold = state.config['conf_threshold']\n",
        "        iou_threshold = state.config['iou_threshold']\n",
        "        max_det = state.config['max_det']\n",
        "        resolution = state.config['resolution']\n",
        "\n",
        "        new_w, new_h = state.scaled_dims\n",
        "        step_size = tile_size - overlap\n",
        "\n",
        "        # Generate tile positions\n",
        "        tile_positions = []\n",
        "        for y in range(0, new_h, step_size):\n",
        "            for x in range(0, new_w, step_size):\n",
        "                x_end = min(x + tile_size, new_w)\n",
        "                y_end = min(y + tile_size, new_h)\n",
        "                tile_positions.append((x, y, x_end, y_end))\n",
        "\n",
        "        print(f\"Processing {len(tile_positions)} tiles with YOLO...\")\n",
        "\n",
        "        # Always use sequential for Colab - Dask causes issues with GPU memory\n",
        "        state.boxes_list = self._process_tiles_sequential(\n",
        "            state, tile_positions, tile_size,\n",
        "            conf_threshold, iou_threshold, max_det, resolution\n",
        "        )\n",
        "\n",
        "        print(f\"YOLO detected {len(state.boxes_list)} boxes before NMS\")\n",
        "        return state\n",
        "\n",
        "    def _process_tiles_sequential(self, state, tile_positions, tile_size, conf, iou, max_det, resolution):\n",
        "        \"\"\"Process tiles sequentially (stable for Colab)\"\"\"\n",
        "        all_boxes = []\n",
        "\n",
        "        # Process in smaller batches to avoid memory issues\n",
        "        batch_size = min(8, len(tile_positions))\n",
        "\n",
        "        for batch_idx in range(0, len(tile_positions), batch_size):\n",
        "            batch = tile_positions[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            for x, y, x_end, y_end in tqdm(batch, desc=f\"YOLO Batch {batch_idx//batch_size + 1}\"):\n",
        "                win_w = x_end - x\n",
        "                win_h = y_end - y\n",
        "                tile = state.resized_image[y:y_end, x:x_end]\n",
        "\n",
        "                if tile.shape[:2] != (tile_size, tile_size):\n",
        "                    tile = cv2.resize(tile, (tile_size, tile_size))\n",
        "                    scale_x = win_w / tile_size\n",
        "                    scale_y = win_h / tile_size\n",
        "                else:\n",
        "                    scale_x = scale_y = 1.0\n",
        "\n",
        "                tile = np.ascontiguousarray(tile)\n",
        "\n",
        "                results = state.model.predict(\n",
        "                    source=tile,\n",
        "                    conf=conf,\n",
        "                    iou=iou,\n",
        "                    max_det=max_det,\n",
        "                    save_txt=False,\n",
        "                    save_conf=True,\n",
        "                    verbose=False\n",
        "                )\n",
        "\n",
        "                if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
        "                    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "                    confidences = results[0].boxes.conf.cpu().numpy()\n",
        "\n",
        "                    for i in range(len(boxes)):\n",
        "                        x1, y1, x2, y2 = boxes[i]\n",
        "                        x1_orig = int((x1 * scale_x + x) / resolution)\n",
        "                        y1_orig = int((y1 * scale_y + y) / resolution)\n",
        "                        x2_orig = int((x2 * scale_x + x) / resolution)\n",
        "                        y2_orig = int((y2 * scale_y + y) / resolution)\n",
        "\n",
        "                        all_boxes.append({\n",
        "                            'bbox': [x1_orig, y1_orig, x2_orig, y2_orig],\n",
        "                            'confidence': float(confidences[i])\n",
        "                        })\n",
        "\n",
        "        return all_boxes\n",
        "\n",
        "\n",
        "class NMSTransition(StateTransition):\n",
        "    \"\"\"Transition: Apply Non-Maximum Suppression\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        if not state.config.get('apply_nms', True) or len(state.boxes_list) == 0:\n",
        "            return state\n",
        "\n",
        "        nms_iou = state.config.get('nms_iou', 0.3)\n",
        "        original_count = len(state.boxes_list)\n",
        "        state.boxes_list = self._apply_nms(state.boxes_list, nms_iou)\n",
        "        print(f\"After NMS: {original_count} → {len(state.boxes_list)} boxes\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def _apply_nms(boxes_list, iou_threshold):\n",
        "        if len(boxes_list) == 0:\n",
        "            return boxes_list\n",
        "\n",
        "        boxes_list = sorted(boxes_list, key=lambda x: x['confidence'], reverse=True)\n",
        "        keep = []\n",
        "        removed = set()\n",
        "\n",
        "        for i in range(len(boxes_list)):\n",
        "            if i in removed:\n",
        "                continue\n",
        "\n",
        "            keep.append(boxes_list[i])\n",
        "            box_i = boxes_list[i]['bbox']\n",
        "\n",
        "            for j in range(i + 1, len(boxes_list)):\n",
        "                if j in removed:\n",
        "                    continue\n",
        "\n",
        "                box_j = boxes_list[j]['bbox']\n",
        "                iou = NMSTransition._calculate_iou(box_i, box_j)\n",
        "\n",
        "                if iou > iou_threshold:\n",
        "                    removed.add(j)\n",
        "\n",
        "        return keep\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_iou(box1, box2):\n",
        "        x1_inter = max(box1[0], box2[0])\n",
        "        y1_inter = max(box1[1], box2[1])\n",
        "        x2_inter = min(box1[2], box2[2])\n",
        "        y2_inter = min(box1[3], box2[3])\n",
        "\n",
        "        if x2_inter < x1_inter or y2_inter < y1_inter:\n",
        "            return 0.0\n",
        "\n",
        "        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
        "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "        union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "        return inter_area / union_area if union_area > 0 else 0.0\n",
        "\n",
        "\n",
        "class SAMInitTransition(StateTransition):\n",
        "    \"\"\"Transition: Initialize SAM model\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        sam_checkpoint = state.config['sam_checkpoint']\n",
        "        model_type = state.config.get('sam_model_type', 'vit_h')\n",
        "        device = state.config.get('device', 'cuda')\n",
        "\n",
        "        if not os.path.exists(sam_checkpoint):\n",
        "            raise FileNotFoundError(f\"SAM checkpoint not found at {sam_checkpoint}\")\n",
        "\n",
        "        print(f\"\\nLoading SAM model ({model_type})...\")\n",
        "\n",
        "        if device == \"cuda\" and not torch.cuda.is_available():\n",
        "            print(\"CUDA not available, using CPU\")\n",
        "            device = \"cpu\"\n",
        "\n",
        "        sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "        sam.to(device=device)\n",
        "        state.sam_predictor = SamPredictor(sam)\n",
        "\n",
        "        print(f\"SAM model loaded on {device}\")\n",
        "        return state\n",
        "\n",
        "\n",
        "class SAMSegmentationTransition(StateTransition):\n",
        "    \"\"\"Transition: Perform SAM segmentation with optimization\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        print(f\"Segmenting {len(state.boxes_list)} objects with SAM...\")\n",
        "\n",
        "        h, w = state.full_image.shape[:2]\n",
        "        max_image_size = state.config.get('sam_max_image_size', 4096)\n",
        "        use_chunked = max(h, w) > max_image_size\n",
        "\n",
        "        if use_chunked:\n",
        "            print(f\"⚠ Large image ({w}x{h}), using optimized chunked processing\")\n",
        "            state.segmentation_results = self._segment_chunked_optimized(state, max_image_size)\n",
        "        else:\n",
        "            print(f\"Setting full image for SAM ({w}x{h})\")\n",
        "            state.sam_predictor.set_image(state.full_image)\n",
        "            state.segmentation_results = self._segment_full_optimized(state)\n",
        "\n",
        "        print(f\"SAM segmentation complete: {len(state.segmentation_results)} masks generated\")\n",
        "        return state\n",
        "\n",
        "    def _segment_full_optimized(self, state):\n",
        "        \"\"\"Optimized full image segmentation\"\"\"\n",
        "        results = []\n",
        "\n",
        "        # Sort boxes by Y coordinate for better memory access patterns\n",
        "        sorted_boxes = sorted(state.boxes_list, key=lambda x: x['bbox'][1])\n",
        "\n",
        "        # Process in batches to avoid GPU memory issues\n",
        "        batch_size = min(50, len(sorted_boxes))\n",
        "\n",
        "        for i in range(0, len(sorted_boxes), batch_size):\n",
        "            batch = sorted_boxes[i:i + batch_size]\n",
        "\n",
        "            for box_info in tqdm(batch, desc=f\"SAM Batch {i//batch_size + 1}\"):\n",
        "                bbox = box_info['bbox']\n",
        "                confidence = box_info['confidence']\n",
        "                input_box = np.array(bbox)\n",
        "\n",
        "                masks, scores, _ = state.sam_predictor.predict(\n",
        "                    point_coords=None,\n",
        "                    point_labels=None,\n",
        "                    box=input_box[None, :],\n",
        "                    multimask_output=False,\n",
        "                )\n",
        "\n",
        "                mask = masks[0]\n",
        "                mask_coords = self._mask_to_polygon(mask)\n",
        "\n",
        "                if mask_coords is not None and len(mask_coords) >= 3:\n",
        "                    results.append({\n",
        "                        'mask': mask,\n",
        "                        'coords': mask_coords,\n",
        "                        'bbox': bbox,\n",
        "                        'confidence': confidence,\n",
        "                        'sam_score': float(scores[0])\n",
        "                    })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _segment_chunked_optimized(self, state, max_size):\n",
        "        \"\"\"Optimized chunked processing\"\"\"\n",
        "        h, w = state.full_image.shape[:2]\n",
        "        results = []\n",
        "\n",
        "        # Group boxes by spatial proximity to share crops\n",
        "        grid_size = 200\n",
        "        box_groups = {}\n",
        "\n",
        "        for idx, box_info in enumerate(state.boxes_list):\n",
        "            bbox = box_info['bbox']\n",
        "            grid_x = bbox[0] // grid_size\n",
        "            grid_y = bbox[1] // grid_size\n",
        "            key = (grid_x, grid_y)\n",
        "\n",
        "            if key not in box_groups:\n",
        "                box_groups[key] = []\n",
        "            box_groups[key].append((idx, box_info))\n",
        "\n",
        "        print(f\"Grouped {len(state.boxes_list)} boxes into {len(box_groups)} spatial groups\")\n",
        "\n",
        "        # Process each group\n",
        "        for group_idx, (key, group_boxes) in enumerate(tqdm(box_groups.items(), desc=\"SAM Groups\")):\n",
        "            if len(group_boxes) == 0:\n",
        "                continue\n",
        "\n",
        "            # Find bounding box for the entire group\n",
        "            min_x = min(box[1]['bbox'][0] for box in group_boxes)\n",
        "            min_y = min(box[1]['bbox'][1] for box in group_boxes)\n",
        "            max_x = max(box[1]['bbox'][2] for box in group_boxes)\n",
        "            max_y = max(box[1]['bbox'][3] for box in group_boxes)\n",
        "\n",
        "            # Add padding\n",
        "            box_w = max_x - min_x\n",
        "            box_h = max_y - min_y\n",
        "            pad_factor = 1.5\n",
        "            pad_w = int(box_w * (pad_factor - 1) / 2)\n",
        "            pad_h = int(box_h * (pad_factor - 1) / 2)\n",
        "\n",
        "            crop_x1 = max(0, min_x - pad_w)\n",
        "            crop_y1 = max(0, min_y - pad_h)\n",
        "            crop_x2 = min(w, max_x + pad_w)\n",
        "            crop_y2 = min(h, max_y + pad_h)\n",
        "\n",
        "            # Ensure crop isn't too large\n",
        "            crop_w = crop_x2 - crop_x1\n",
        "            crop_h = crop_y2 - crop_y1\n",
        "\n",
        "            if crop_w > max_size or crop_h > max_size:\n",
        "                pad_w = min(pad_w, (max_size - box_w) // 2)\n",
        "                pad_h = min(pad_h, (max_size - box_h) // 2)\n",
        "                crop_x1 = max(0, min_x - pad_w)\n",
        "                crop_y1 = max(0, min_y - pad_h)\n",
        "                crop_x2 = min(w, max_x + pad_w)\n",
        "                crop_y2 = min(h, max_y + pad_h)\n",
        "\n",
        "            # Extract and process crop once for the whole group\n",
        "            crop = state.full_image[crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "            state.sam_predictor.set_image(crop)\n",
        "\n",
        "            # Process all boxes in the group\n",
        "            for idx, box_info in group_boxes:\n",
        "                bbox = box_info['bbox']\n",
        "                confidence = box_info['confidence']\n",
        "\n",
        "                bbox_in_crop = [\n",
        "                    bbox[0] - crop_x1,\n",
        "                    bbox[1] - crop_y1,\n",
        "                    bbox[2] - crop_x1,\n",
        "                    bbox[3] - crop_y1\n",
        "                ]\n",
        "\n",
        "                input_box = np.array(bbox_in_crop)\n",
        "\n",
        "                try:\n",
        "                    masks, scores, _ = state.sam_predictor.predict(\n",
        "                        point_coords=None,\n",
        "                        point_labels=None,\n",
        "                        box=input_box[None, :],\n",
        "                        multimask_output=False,\n",
        "                    )\n",
        "\n",
        "                    mask = masks[0]\n",
        "                    mask_coords_crop = self._mask_to_polygon(mask)\n",
        "\n",
        "                    if mask_coords_crop is not None and len(mask_coords_crop) >= 3:\n",
        "                        mask_coords = mask_coords_crop + np.array([crop_x1, crop_y1])\n",
        "\n",
        "                        results.append({\n",
        "                            'mask': None,\n",
        "                            'coords': mask_coords,\n",
        "                            'bbox': bbox,\n",
        "                            'confidence': confidence,\n",
        "                            'sam_score': float(scores[0])\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return results\n",
        "\n",
        "    @staticmethod\n",
        "    def _mask_to_polygon(mask):\n",
        "        contours, _ = cv2.findContours(\n",
        "            mask.astype(np.uint8),\n",
        "            cv2.RETR_EXTERNAL,\n",
        "            cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            return None\n",
        "\n",
        "        contour = max(contours, key=cv2.contourArea)\n",
        "        epsilon = 0.001 * cv2.arcLength(contour, True)\n",
        "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "\n",
        "        return approx.reshape(-1, 2)\n",
        "\n",
        "\n",
        "class CreateGeoDataFrameTransition(StateTransition):\n",
        "    \"\"\"Transition: Create GeoDataFrame from segmentation results\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        print(f\"Creating GeoDataFrame from {len(state.segmentation_results)} results...\")\n",
        "\n",
        "        all_polygons = []\n",
        "        detection_info = []\n",
        "\n",
        "        # Process in batches to avoid memory issues\n",
        "        batch_size = min(100, len(state.segmentation_results))\n",
        "\n",
        "        for i in range(0, len(state.segmentation_results), batch_size):\n",
        "            batch = state.segmentation_results[i:i + batch_size]\n",
        "\n",
        "            for result in tqdm(batch, desc=f\"Polygons Batch {i//batch_size + 1}\"):\n",
        "                coords = result['coords']\n",
        "\n",
        "                try:\n",
        "                    polygon_shapely = Polygon(coords)\n",
        "                    if not polygon_shapely.is_valid:\n",
        "                        polygon_shapely = polygon_shapely.buffer(0)\n",
        "                    area_pixels = polygon_shapely.area\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                geo_coords = []\n",
        "                for px, py in coords:\n",
        "                    geo_x, geo_y = rasterio.transform.xy(state.transform, py, px)\n",
        "                    geo_coords.append((geo_x, geo_y))\n",
        "\n",
        "                try:\n",
        "                    geo_polygon = Polygon(geo_coords)\n",
        "                    if not geo_polygon.is_valid:\n",
        "                        geo_polygon = geo_polygon.buffer(0)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                all_polygons.append(geo_polygon)\n",
        "                detection_info.append({\n",
        "                    'centroid_x': coords[:, 0].mean(),\n",
        "                    'centroid_y': coords[:, 1].mean(),\n",
        "                    'area_pixels': area_pixels,\n",
        "                    'confidence': result['confidence'],\n",
        "                    'sam_score': result['sam_score'],\n",
        "                    'num_points': len(coords)\n",
        "                })\n",
        "\n",
        "        if len(all_polygons) == 0:\n",
        "            state.gdf = gpd.GeoDataFrame()\n",
        "            state.detection_info = []\n",
        "            return state\n",
        "\n",
        "        state.gdf = gpd.GeoDataFrame({\n",
        "            'geometry': all_polygons,\n",
        "            'area_pixels': [d['area_pixels'] for d in detection_info],\n",
        "            'confidence': [d['confidence'] for d in detection_info],\n",
        "            'sam_score': [d['sam_score'] for d in detection_info],\n",
        "            'centroid_x': [d['centroid_x'] for d in detection_info],\n",
        "            'centroid_y': [d['centroid_y'] for d in detection_info],\n",
        "            'num_points': [d['num_points'] for d in detection_info]\n",
        "        }, crs=state.crs)\n",
        "\n",
        "        state.detection_info = detection_info\n",
        "        print(f\"✓ Created GeoDataFrame with {len(state.gdf)} features\")\n",
        "        return state\n",
        "\n",
        "\n",
        "class VisualizationTransition(StateTransition):\n",
        "    \"\"\"Transition: Visualize results\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        output_dir = state.config['output_dir']\n",
        "        height, width = state.full_image.shape[:2]\n",
        "\n",
        "        max_viz_size = state.config.get('max_viz_size', 2048)\n",
        "\n",
        "        if max(width, height) > max_viz_size:\n",
        "            print(f\"⚠ Large image ({width}x{height}), downsampling for visualization...\")\n",
        "            scale = max_viz_size / max(width, height)\n",
        "            viz_width = int(width * scale)\n",
        "            viz_height = int(height * scale)\n",
        "            viz_image = cv2.resize(state.full_image, (viz_width, viz_height),\n",
        "                                  interpolation=cv2.INTER_AREA)\n",
        "            print(f\"  Visualization size: {viz_width}x{viz_height}\")\n",
        "        else:\n",
        "            viz_image = state.full_image\n",
        "            viz_width, viz_height = width, height\n",
        "            scale = 1.0\n",
        "\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 8), dpi=150)\n",
        "        ax.imshow(viz_image)\n",
        "        ax.set_title(f'YOLO + SAM Segmentation - {len(state.segmentation_results)} Trees Detected',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        if state.segmentation_results:\n",
        "            print(f\"  Drawing {len(state.segmentation_results)} detections...\")\n",
        "\n",
        "            # Draw in batches to avoid memory issues\n",
        "            batch_size = min(500, len(state.segmentation_results))\n",
        "            for i in range(0, len(state.segmentation_results), batch_size):\n",
        "                batch = state.segmentation_results[i:i + batch_size]\n",
        "\n",
        "                for result in batch:\n",
        "                    coords = result['coords'] * scale\n",
        "                    coords_closed = np.vstack([coords, coords[0]])\n",
        "\n",
        "                    ax.plot(coords_closed[:, 0], coords_closed[:, 1],\n",
        "                           'lime', linewidth=1, alpha=0.7)\n",
        "\n",
        "                    polygon = MPLPolygon(coords, closed=True,\n",
        "                                       facecolor='lime', alpha=0.2,\n",
        "                                       edgecolor='lime', linewidth=1)\n",
        "                    ax.add_patch(polygon)\n",
        "\n",
        "            avg_conf = np.mean([r['confidence'] for r in state.segmentation_results]) if state.segmentation_results else 0\n",
        "            avg_sam = np.mean([r['sam_score'] for r in state.segmentation_results]) if state.segmentation_results else 0\n",
        "\n",
        "            stats_text = (f\"Total Trees: {len(state.segmentation_results)}\\n\"\n",
        "                         f\"Avg YOLO Conf: {avg_conf:.3f}\\n\"\n",
        "                         f\"Avg SAM Score: {avg_sam:.3f}\")\n",
        "\n",
        "            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
        "                   fontsize=12, verticalalignment='top',\n",
        "                   bbox=dict(boxstyle=\"round\", fc=\"white\", alpha=0.9, pad=0.5))\n",
        "\n",
        "        ax.set_xlim(0, viz_width)\n",
        "        ax.set_ylim(viz_height, 0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_dpi = state.config.get('save_dpi', 150)\n",
        "        image_name = state.config.get('image_name', 'output')\n",
        "        output_path = f'{output_dir}/{image_name}_segmentation.png'\n",
        "        plt.savefig(output_path, dpi=save_dpi, bbox_inches='tight')\n",
        "\n",
        "        plt.close('all')\n",
        "\n",
        "        print(f\"✓ Visualization saved to {output_path}\")\n",
        "        return state\n",
        "\n",
        "\n",
        "class SaveResultsTransition(StateTransition):\n",
        "    \"\"\"Transition: Save results to files\"\"\"\n",
        "\n",
        "    def __call__(self, state):\n",
        "        if state.gdf is None or len(state.gdf) == 0:\n",
        "            print(\"No detections to save\")\n",
        "            return state\n",
        "\n",
        "        output_dir = state.config['output_dir']\n",
        "        image_name = state.config.get('image_name', 'output')\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save GeoJSON\n",
        "        if state.gdf.crs is not None:\n",
        "            geojson_path = f'{output_dir}/{image_name}_detections.geojson'\n",
        "            try:\n",
        "                state.gdf.to_file(geojson_path, driver='GeoJSON')\n",
        "                print(f\"✓ Saved GeoJSON to {geojson_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Failed to save GeoJSON: {e}\")\n",
        "        else:\n",
        "            print(\"⚠ Skipping GeoJSON (no CRS)\")\n",
        "\n",
        "        # Save CSV\n",
        "        csv_path = f'{output_dir}/{image_name}_detections.csv'\n",
        "        try:\n",
        "            state.gdf.drop(columns='geometry').to_csv(csv_path, index=False)\n",
        "            print(f\"✓ Saved CSV to {csv_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Failed to save CSV: {e}\")\n",
        "\n",
        "        # Save statistics\n",
        "        if state.detection_info:\n",
        "            stats = {\n",
        "                'total_detections': len(state.gdf),\n",
        "                'average_area': float(np.mean([d['area_pixels'] for d in state.detection_info])),\n",
        "                'median_area': float(np.median([d['area_pixels'] for d in state.detection_info])),\n",
        "                'average_yolo_confidence': float(np.mean([d['confidence'] for d in state.detection_info])),\n",
        "                'average_sam_score': float(np.mean([d['sam_score'] for d in state.detection_info]))\n",
        "            }\n",
        "\n",
        "            stats_path = f'{output_dir}/{image_name}_statistics.json'\n",
        "            try:\n",
        "                with open(stats_path, 'w') as f:\n",
        "                    json.dump(stats, f, indent=2)\n",
        "                print(f\"✓ Saved statistics to {stats_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Failed to save statistics: {e}\")\n",
        "\n",
        "        # Save processing time\n",
        "        processing_time = time.process_time() - state.start_time\n",
        "        print(f\"✓ Total processing time: {processing_time:.2f} seconds\")\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class DetectionPipeline:\n",
        "    \"\"\"State-space detection pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self, transitions):\n",
        "        self.transitions = transitions\n",
        "\n",
        "    def run(self, initial_state):\n",
        "        state = initial_state\n",
        "\n",
        "        for i, transition in enumerate(self.transitions):\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Transition {i+1}/{len(self.transitions)}: {transition.__class__.__name__}\")\n",
        "            print(f\"{'='*60}\")\n",
        "            state = transition(state)\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "def create_detection_pipeline(config):\n",
        "    \"\"\"Factory function to create the detection pipeline\"\"\"\n",
        "    transitions = [\n",
        "        LoadImageTransition(),\n",
        "        ResizeImageTransition(),\n",
        "        YOLODetectionTransition(),\n",
        "        NMSTransition(),\n",
        "        SAMInitTransition(),\n",
        "        SAMSegmentationTransition(),\n",
        "        CreateGeoDataFrameTransition(),\n",
        "        VisualizationTransition(),\n",
        "        SaveResultsTransition()\n",
        "    ]\n",
        "\n",
        "    return DetectionPipeline(transitions)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example configuration with optimizations for Colab\n",
        "    config = {\n",
        "        # Core detection parameters\n",
        "        'tile_size': 1280,\n",
        "        'overlap': 128,\n",
        "        'resolution': 0.5,\n",
        "        'conf_threshold': 0.20,\n",
        "        'iou_threshold': 0.45,\n",
        "        'max_det': 5000,\n",
        "        'apply_nms': False,\n",
        "        'nms_iou': 0.3,\n",
        "\n",
        "        # SAM parameters\n",
        "        'sam_checkpoint': '/content/drive/MyDrive/AGRI/TreeCrown_Segmentation/models/segmentation/sam_vit_b_01ec64.pth',\n",
        "        'sam_model_type': 'vit_b',\n",
        "        'device': 'cuda',\n",
        "        'sam_max_image_size': 4096,\n",
        "\n",
        "        # Optimization settings\n",
        "        'use_dask': True,\n",
        "        'max_viz_size': 2048,\n",
        "        'save_dpi': 150,\n",
        "\n",
        "        # Output settings\n",
        "        'output_dir': '/content/drive/MyDrive/AGRI/TreeCrown_Segmentation/output',\n",
        "        'image_name': 'test_optimized'\n",
        "    }\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(config['output_dir'], exist_ok=True)\n",
        "\n",
        "    # Load YOLO model\n",
        "    from ultralytics import YOLO\n",
        "    print(\"Loading YOLO model...\")\n",
        "    yolo_model = YOLO('/content/drive/MyDrive/AGRI/TreeCrown_Segmentation/models/detection/yolo11n-100epoch-v2.pt')\n",
        "    print(\"✓ YOLO model loaded\")\n",
        "\n",
        "    # Create initial state\n",
        "    initial_state = DetectionState(\n",
        "         image_path='/content/drive/MyDrive/AGRI/TreeCrown_Segmentation/ortho_872.tif',\n",
        "         model=yolo_model,\n",
        "         config=config\n",
        "    )\n",
        "\n",
        "    # Create and run pipeline\n",
        "    pipeline = create_detection_pipeline(config)\n",
        "\n",
        "    try:\n",
        "        final_state = pipeline.run(initial_state)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"✓ Pipeline completed successfully!\")\n",
        "        print(f\"Final state: {final_state}\")\n",
        "\n",
        "        if final_state.gdf is not None and len(final_state.gdf) > 0:\n",
        "            print(f\"\\nSummary:\")\n",
        "            print(f\"  Detected trees: {len(final_state.gdf)}\")\n",
        "            print(f\"  Output directory: {config['output_dir']}\")\n",
        "            print(f\"  Output files:\")\n",
        "            for ext in ['.geojson', '.csv', '.json', '.png']:\n",
        "                file_path = f\"{config['output_dir']}/{config['image_name']}_*{ext}\"\n",
        "                import glob\n",
        "                files = glob.glob(file_path)\n",
        "                for f in files:\n",
        "                    print(f\"    - {os.path.basename(f)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"✗ Pipeline failed with error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(f\"{'='*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNss--wVzpEA",
        "outputId": "63e31041-eb07-45c5-eca5-02445945ddb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading YOLO model...\n",
            "✓ YOLO model loaded\n",
            "\n",
            "============================================================\n",
            "Transition 1/9: LoadImageTransition\n",
            "============================================================\n",
            "Original image size: 40244x25007\n",
            "\n",
            "============================================================\n",
            "Transition 2/9: ResizeImageTransition\n",
            "============================================================\n",
            "Processing at: 20122x12503 (resolution=0.5)\n",
            "\n",
            "============================================================\n",
            "Transition 3/9: YOLODetectionTransition\n",
            "============================================================\n",
            "Processing 198 tiles with YOLO...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLO Batch 1: 100%|██████████| 8/8 [00:00<00:00,  9.78it/s]\n",
            "YOLO Batch 2: 100%|██████████| 8/8 [00:00<00:00, 73.33it/s]\n",
            "YOLO Batch 3: 100%|██████████| 8/8 [00:00<00:00, 65.50it/s]\n",
            "YOLO Batch 4: 100%|██████████| 8/8 [00:00<00:00, 76.42it/s]\n",
            "YOLO Batch 5: 100%|██████████| 8/8 [00:00<00:00, 78.19it/s]\n",
            "YOLO Batch 6: 100%|██████████| 8/8 [00:00<00:00, 76.05it/s]\n",
            "YOLO Batch 7: 100%|██████████| 8/8 [00:00<00:00, 74.30it/s]\n",
            "YOLO Batch 8: 100%|██████████| 8/8 [00:00<00:00, 78.88it/s]\n",
            "YOLO Batch 9: 100%|██████████| 8/8 [00:00<00:00, 74.04it/s]\n",
            "YOLO Batch 10: 100%|██████████| 8/8 [00:00<00:00, 74.79it/s]\n",
            "YOLO Batch 11: 100%|██████████| 8/8 [00:00<00:00, 80.26it/s]\n",
            "YOLO Batch 12: 100%|██████████| 8/8 [00:00<00:00, 63.28it/s]\n",
            "YOLO Batch 13: 100%|██████████| 8/8 [00:00<00:00, 77.17it/s]\n",
            "YOLO Batch 14: 100%|██████████| 8/8 [00:00<00:00, 75.67it/s]\n",
            "YOLO Batch 15: 100%|██████████| 8/8 [00:00<00:00, 78.17it/s]\n",
            "YOLO Batch 16: 100%|██████████| 8/8 [00:00<00:00, 75.13it/s]\n",
            "YOLO Batch 17: 100%|██████████| 8/8 [00:00<00:00, 76.95it/s]\n",
            "YOLO Batch 18: 100%|██████████| 8/8 [00:00<00:00, 73.91it/s]\n",
            "YOLO Batch 19: 100%|██████████| 8/8 [00:00<00:00, 75.16it/s]\n",
            "YOLO Batch 20: 100%|██████████| 8/8 [00:00<00:00, 76.47it/s]\n",
            "YOLO Batch 21: 100%|██████████| 8/8 [00:00<00:00, 72.82it/s]\n",
            "YOLO Batch 22: 100%|██████████| 8/8 [00:00<00:00, 63.41it/s]\n",
            "YOLO Batch 23: 100%|██████████| 8/8 [00:00<00:00, 68.78it/s]\n",
            "YOLO Batch 24: 100%|██████████| 8/8 [00:00<00:00, 61.49it/s]\n",
            "YOLO Batch 25: 100%|██████████| 6/6 [00:00<00:00, 63.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO detected 2221 boxes before NMS\n",
            "\n",
            "============================================================\n",
            "Transition 4/9: NMSTransition\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Transition 5/9: SAMInitTransition\n",
            "============================================================\n",
            "\n",
            "Loading SAM model (vit_b)...\n",
            "SAM model loaded on cuda\n",
            "\n",
            "============================================================\n",
            "Transition 6/9: SAMSegmentationTransition\n",
            "============================================================\n",
            "Segmenting 2221 objects with SAM...\n",
            "⚠ Large image (40244x25007), using optimized chunked processing\n",
            "Grouped 2221 boxes into 1896 spatial groups\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SAM Groups: 100%|██████████| 1896/1896 [13:38<00:00,  2.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAM segmentation complete: 2221 masks generated\n",
            "\n",
            "============================================================\n",
            "Transition 7/9: CreateGeoDataFrameTransition\n",
            "============================================================\n",
            "Creating GeoDataFrame from 2221 results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Polygons Batch 1: 100%|██████████| 100/100 [00:00<00:00, 101.73it/s]\n",
            "Polygons Batch 2: 100%|██████████| 100/100 [00:01<00:00, 92.82it/s]\n",
            "Polygons Batch 3: 100%|██████████| 100/100 [00:00<00:00, 152.99it/s]\n",
            "Polygons Batch 4: 100%|██████████| 100/100 [00:00<00:00, 149.78it/s]\n",
            "Polygons Batch 5: 100%|██████████| 100/100 [00:00<00:00, 167.43it/s]\n",
            "Polygons Batch 6: 100%|██████████| 100/100 [00:00<00:00, 158.28it/s]\n",
            "Polygons Batch 7: 100%|██████████| 100/100 [00:00<00:00, 141.83it/s]\n",
            "Polygons Batch 8: 100%|██████████| 100/100 [00:00<00:00, 147.46it/s]\n",
            "Polygons Batch 9: 100%|██████████| 100/100 [00:00<00:00, 139.86it/s]\n",
            "Polygons Batch 10: 100%|██████████| 100/100 [00:00<00:00, 136.82it/s]\n",
            "Polygons Batch 11: 100%|██████████| 100/100 [00:00<00:00, 147.94it/s]\n",
            "Polygons Batch 12: 100%|██████████| 100/100 [00:00<00:00, 145.34it/s]\n",
            "Polygons Batch 13: 100%|██████████| 100/100 [00:00<00:00, 150.25it/s]\n",
            "Polygons Batch 14: 100%|██████████| 100/100 [00:00<00:00, 140.96it/s]\n",
            "Polygons Batch 15: 100%|██████████| 100/100 [00:00<00:00, 142.81it/s]\n",
            "Polygons Batch 16: 100%|██████████| 100/100 [00:00<00:00, 135.27it/s]\n",
            "Polygons Batch 17: 100%|██████████| 100/100 [00:00<00:00, 108.96it/s]\n",
            "Polygons Batch 18: 100%|██████████| 100/100 [00:01<00:00, 80.48it/s]\n",
            "Polygons Batch 19: 100%|██████████| 100/100 [00:01<00:00, 89.90it/s]\n",
            "Polygons Batch 20: 100%|██████████| 100/100 [00:01<00:00, 96.25it/s]\n",
            "Polygons Batch 21: 100%|██████████| 100/100 [00:00<00:00, 139.91it/s]\n",
            "Polygons Batch 22: 100%|██████████| 100/100 [00:00<00:00, 144.86it/s]\n",
            "Polygons Batch 23: 100%|██████████| 21/21 [00:00<00:00, 159.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created GeoDataFrame with 2221 features\n",
            "\n",
            "============================================================\n",
            "Transition 8/9: VisualizationTransition\n",
            "============================================================\n",
            "⚠ Large image (40244x25007), downsampling for visualization...\n",
            "  Visualization size: 2048x1272\n",
            "  Drawing 2221 detections...\n",
            "✓ Visualization saved to /content/drive/MyDrive/AGRI/TreeCrown_Segmentation/output/test_optimized_segmentation.png\n",
            "\n",
            "============================================================\n",
            "Transition 9/9: SaveResultsTransition\n",
            "============================================================\n",
            "✓ Saved GeoJSON to /content/drive/MyDrive/AGRI/TreeCrown_Segmentation/output/test_optimized_detections.geojson\n",
            "✓ Saved CSV to /content/drive/MyDrive/AGRI/TreeCrown_Segmentation/output/test_optimized_detections.csv\n",
            "✓ Saved statistics to /content/drive/MyDrive/AGRI/TreeCrown_Segmentation/output/test_optimized_statistics.json\n",
            "✓ Total processing time: 881.27 seconds\n",
            "\n",
            "============================================================\n",
            "✓ Pipeline completed successfully!\n",
            "Final state: DetectionState(boxes=2221, segments=2221, gdf_size=2221)\n",
            "\n",
            "Summary:\n",
            "  Detected trees: 2221\n",
            "  Output directory: /content/drive/MyDrive/AGRI/TreeCrown_Segmentation/output\n",
            "  Output files:\n",
            "    - test_optimized_detections.geojson\n",
            "    - test_optimized_detections.csv\n",
            "    - test_optimized_statistics.json\n",
            "    - test_optimized_segmentation.png\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wj-hc2Kq43Y1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}